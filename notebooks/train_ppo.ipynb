{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Notebook\n",
    "## Train Proximal Policy Optimization Agent for Trade Execution\n",
    "\n",
    "This notebook trains a Reinforcement Learning agent using PPO (Proximal Policy Optimization) for optimal trade execution:\n",
    "1. **Custom Gym Environment**: Trading execution environment with realistic market dynamics\n",
    "2. **PPO Agent**: Stable-Baselines3 PPO implementation\n",
    "3. **Reward Engineering**: PnL-based rewards with transaction cost penalties\n",
    "4. **Training Loop**: Multi-episode training with performance monitoring\n",
    "5. **Policy Evaluation**: Backtesting and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from algos.core.exec_rl import TradingExecutionEnv, ExecutionRL, ExecutionTrainingCallback\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Execution config: {config.get('execution', {})}\")\n",
    "\n",
    "# Environment parameters\n",
    "env_config = {\n",
    "    'max_position_size': 10000.0,      # Maximum position size\n",
    "    'transaction_cost': 0.0005,        # 5 bps transaction cost\n",
    "    'impact_cost': 0.001,              # 10 bps market impact\n",
    "    'max_steps': 100                   # Maximum steps per episode\n",
    "}\n",
    "\n",
    "print(f\"Environment config: {env_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test environment\n",
    "def make_env():\n",
    "    env = TradingExecutionEnv(**env_config)\n",
    "    return env\n",
    "\n",
    "# Test environment\n",
    "test_env = make_env()\n",
    "check_env(test_env)\n",
    "\n",
    "print(\"Environment check passed!\")\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "\n",
    "# Test environment interaction\n",
    "obs, info = test_env.reset()\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "print(f\"Initial info: {info}\")\n",
    "\n",
    "# Take a random action\n",
    "action = test_env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "print(f\"\\nAfter random action {action}:\")\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingCallback(BaseCallback):\n",
    "    \"\"\"Custom callback for trading PPO training.\"\"\"\n",
    "    \n",
    "    def __init__(self, check_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.execution_costs = []\n",
    "        \n",
    "    def _on_step(self):\n",
    "        \"\"\"Called at each step.\"\"\"\n",
    "        \n",
    "        # Log episode completion\n",
    "        if 'episode' in self.locals.get('infos', [{}])[0]:\n",
    "            episode_info = self.locals['infos'][0]['episode']\n",
    "            self.episode_rewards.append(episode_info['r'])\n",
    "            self.episode_lengths.append(episode_info['l'])\n",
    "            \n",
    "            # Calculate execution cost if available\n",
    "            if 'total_cost' in self.locals['infos'][0]:\n",
    "                self.execution_costs.append(self.locals['infos'][0]['total_cost'])\n",
    "        \n",
    "        # Periodic logging\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            if len(self.episode_rewards) > 0:\n",
    "                recent_rewards = self.episode_rewards[-100:]  # Last 100 episodes\n",
    "                recent_lengths = self.episode_lengths[-100:]\n",
    "                \n",
    "                avg_reward = np.mean(recent_rewards)\n",
    "                avg_length = np.mean(recent_lengths)\n",
    "                \n",
    "                self.logger.record(\"rollout/ep_rew_mean\", avg_reward)\n",
    "                self.logger.record(\"rollout/ep_len_mean\", avg_length)\n",
    "                \n",
    "                if len(self.execution_costs) > 0:\n",
    "                    avg_cost = np.mean(self.execution_costs[-100:])\n",
    "                    self.logger.record(\"trading/avg_execution_cost\", avg_cost)\n",
    "                \n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Step {self.n_calls}: Avg Reward = {avg_reward:.4f}, \"\n",
    "                          f\"Avg Length = {avg_length:.1f}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_training_metrics(self):\n",
    "        \"\"\"Get training metrics for analysis.\"\"\"\n",
    "        return {\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'episode_lengths': self.episode_lengths,\n",
    "            'execution_costs': self.execution_costs\n",
    "        }\n",
    "\n",
    "print(\"Custom callback defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment for training\n",
    "env = make_env()\n",
    "env = Monitor(env)  # Monitor wrapper for logging\n",
    "\n",
    "# PPO hyperparameters\n",
    "ppo_config = {\n",
    "    'policy': 'MlpPolicy',\n",
    "    'learning_rate': 3e-4,\n",
    "    'n_steps': 2048,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 0.01,\n",
    "    'vf_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'verbose': 1,\n",
    "    'tensorboard_log': './logs/ppo_execution/'\n",
    "}\n",
    "\n",
    "print(\"PPO Configuration:\")\n",
    "for key, value in ppo_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create PPO model\n",
    "model = PPO(\n",
    "    env=env,\n",
    "    **ppo_config\n",
    ")\n",
    "\n",
    "print(f\"\\nPPO model created successfully\")\n",
    "print(f\"Policy network: {model.policy}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "TOTAL_TIMESTEPS = 100000  # Adjust based on computational resources\n",
    "EVAL_FREQ = 5000\n",
    "SAVE_FREQ = 10000\n",
    "\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"  Evaluation frequency: {EVAL_FREQ:,}\")\n",
    "print(f\"  Save frequency: {SAVE_FREQ:,}\")\n",
    "\n",
    "# Create callbacks\n",
    "training_callback = TradingCallback(check_freq=1000)\n",
    "\n",
    "# Evaluation environment\n",
    "eval_env = Monitor(make_env())\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='../policies/',\n",
    "    log_path='../logs/',\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    n_eval_episodes=10\n",
    ")\n",
    "\n",
    "callbacks = [training_callback, eval_callback]\n",
    "\n",
    "print(\"Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting PPO training...\")\n",
    "print(f\"This may take several minutes for {TOTAL_TIMESTEPS:,} timesteps\")\n",
    "\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        callback=callbacks,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training metrics\n",
    "training_metrics = training_callback.get_training_metrics()\n",
    "\n",
    "print(\"Training Statistics:\")\n",
    "print(f\"  Total episodes: {len(training_metrics['episode_rewards'])}\")\n",
    "print(f\"  Average reward: {np.mean(training_metrics['episode_rewards']):.4f}\")\n",
    "print(f\"  Reward std: {np.std(training_metrics['episode_rewards']):.4f}\")\n",
    "print(f\"  Average episode length: {np.mean(training_metrics['episode_lengths']):.1f}\")\n",
    "\n",
    "if training_metrics['execution_costs']:\n",
    "    print(f\"  Average execution cost: {np.mean(training_metrics['execution_costs']):.6f}\")\n",
    "\n",
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "rewards = training_metrics['episode_rewards']\n",
    "if len(rewards) > 0:\n",
    "    axes[0, 0].plot(rewards, alpha=0.7)\n",
    "    if len(rewards) > 50:\n",
    "        rolling_mean = pd.Series(rewards).rolling(50).mean()\n",
    "        axes[0, 0].plot(rolling_mean, color='red', linewidth=2, label='50-episode average')\n",
    "        axes[0, 0].legend()\n",
    "    axes[0, 0].set_title('Episode Rewards')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Reward')\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "lengths = training_metrics['episode_lengths']\n",
    "if len(lengths) > 0:\n",
    "    axes[0, 1].plot(lengths, alpha=0.7, color='green')\n",
    "    if len(lengths) > 50:\n",
    "        rolling_mean = pd.Series(lengths).rolling(50).mean()\n",
    "        axes[0, 1].plot(rolling_mean, color='darkgreen', linewidth=2, label='50-episode average')\n",
    "        axes[0, 1].legend()\n",
    "    axes[0, 1].set_title('Episode Lengths')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Steps')\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "# Reward distribution\n",
    "if len(rewards) > 0:\n",
    "    axes[1, 0].hist(rewards, bins=30, alpha=0.7, color='blue')\n",
    "    axes[1, 0].axvline(np.mean(rewards), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(rewards):.3f}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_title('Reward Distribution')\n",
    "    axes[1, 0].set_xlabel('Reward')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Execution costs (if available)\n",
    "costs = training_metrics['execution_costs']\n",
    "if len(costs) > 0:\n",
    "    axes[1, 1].plot(costs, alpha=0.7, color='orange')\n",
    "    if len(costs) > 50:\n",
    "        rolling_mean = pd.Series(costs).rolling(50).mean()\n",
    "        axes[1, 1].plot(rolling_mean, color='darkorange', linewidth=2, label='50-episode average')\n",
    "        axes[1, 1].legend()\n",
    "    axes[1, 1].set_title('Execution Costs')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Cost')\n",
    "    axes[1, 1].grid(True)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No execution cost data', \n",
    "                   horizontalalignment='center', verticalalignment='center',\n",
    "                   transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Execution Costs (No Data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Policy Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained policy\n",
    "print(\"Testing trained policy...\")\n",
    "\n",
    "test_episodes = 10\n",
    "test_results = []\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    episode_actions = []\n",
    "    episode_positions = []\n",
    "    \n",
    "    terminated = truncated = False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        episode_actions.append(action.copy())\n",
    "        episode_positions.append(info.get('current_position', 0))\n",
    "    \n",
    "    test_results.append({\n",
    "        'episode': episode,\n",
    "        'reward': episode_reward,\n",
    "        'steps': episode_steps,\n",
    "        'final_position': info.get('current_position', 0),\n",
    "        'total_pnl': info.get('total_pnl', 0),\n",
    "        'total_cost': info.get('total_cost', 0),\n",
    "        'actions': episode_actions,\n",
    "        'positions': episode_positions\n",
    "    })\n",
    "    \n",
    "    print(f\"Episode {episode}: Reward = {episode_reward:.4f}, \"\n",
    "          f\"Steps = {episode_steps}, PnL = {info.get('total_pnl', 0):.4f}\")\n",
    "\n",
    "# Analyze test results\n",
    "test_rewards = [r['reward'] for r in test_results]\n",
    "test_pnls = [r['total_pnl'] for r in test_results]\n",
    "test_costs = [r['total_cost'] for r in test_results]\n",
    "\n",
    "print(f\"\\nTest Results Summary:\")\n",
    "print(f\"  Average reward: {np.mean(test_rewards):.4f} ± {np.std(test_rewards):.4f}\")\n",
    "print(f\"  Average PnL: {np.mean(test_pnls):.4f} ± {np.std(test_pnls):.4f}\")\n",
    "print(f\"  Average cost: {np.mean(test_costs):.6f} ± {np.std(test_costs):.6f}\")\n",
    "print(f\"  Success rate: {sum(1 for r in test_rewards if r > 0) / len(test_rewards):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test episode behavior\n",
    "if test_results:\n",
    "    # Select a representative episode\n",
    "    best_episode_idx = np.argmax(test_rewards)\n",
    "    best_episode = test_results[best_episode_idx]\n",
    "    \n",
    "    print(f\"Analyzing best episode (Episode {best_episode_idx}):\")\n",
    "    print(f\"  Reward: {best_episode['reward']:.4f}\")\n",
    "    print(f\"  PnL: {best_episode['total_pnl']:.4f}\")\n",
    "    print(f\"  Steps: {best_episode['steps']}\")\n",
    "    \n",
    "    # Plot episode behavior\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Position evolution\n",
    "    positions = best_episode['positions']\n",
    "    axes[0, 0].plot(positions, marker='o', markersize=3)\n",
    "    axes[0, 0].set_title(f'Position Evolution (Episode {best_episode_idx})')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Position')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Action components\n",
    "    actions = np.array(best_episode['actions'])\n",
    "    if len(actions) > 0:\n",
    "        axes[0, 1].plot(actions[:, 0], label='Size Delta', alpha=0.7)\n",
    "        axes[0, 1].plot(actions[:, 1], label='Order Type', alpha=0.7)\n",
    "        axes[0, 1].plot(actions[:, 2], label='Limit Offset', alpha=0.7)\n",
    "        axes[0, 1].set_title('Action Components')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Action Value')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Test reward distribution\n",
    "    axes[1, 0].hist(test_rewards, bins=10, alpha=0.7, color='green')\n",
    "    axes[1, 0].axvline(np.mean(test_rewards), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(test_rewards):.3f}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_title('Test Episode Rewards')\n",
    "    axes[1, 0].set_xlabel('Reward')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # PnL vs Cost scatter\n",
    "    axes[1, 1].scatter(test_costs, test_pnls, alpha=0.7)\n",
    "    axes[1, 1].set_title('PnL vs Execution Cost')\n",
    "    axes[1, 1].set_xlabel('Execution Cost')\n",
    "    axes[1, 1].set_ylabel('PnL')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Policy evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policies directory\n",
    "os.makedirs('../policies', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "policy_path = '../policies/ppo_policy.zip'\n",
    "model.save(policy_path)\n",
    "print(f\"PPO policy saved to {policy_path}\")\n",
    "\n",
    "# Save training configuration and results\n",
    "training_summary = {\n",
    "    'training_config': {\n",
    "        'total_timesteps': TOTAL_TIMESTEPS,\n",
    "        'ppo_config': ppo_config,\n",
    "        'env_config': env_config\n",
    "    },\n",
    "    'training_results': {\n",
    "        'total_episodes': len(training_metrics['episode_rewards']),\n",
    "        'final_avg_reward': np.mean(training_metrics['episode_rewards'][-100:]) if len(training_metrics['episode_rewards']) >= 100 else np.mean(training_metrics['episode_rewards']),\n",
    "        'final_reward_std': np.std(training_metrics['episode_rewards'][-100:]) if len(training_metrics['episode_rewards']) >= 100 else np.std(training_metrics['episode_rewards']),\n",
    "        'convergence_achieved': len(training_metrics['episode_rewards']) > 0\n",
    "    },\n",
    "    'test_results': {\n",
    "        'test_episodes': test_episodes,\n",
    "        'avg_test_reward': np.mean(test_rewards),\n",
    "        'test_reward_std': np.std(test_rewards),\n",
    "        'avg_test_pnl': np.mean(test_pnls),\n",
    "        'avg_test_cost': np.mean(test_costs),\n",
    "        'success_rate': sum(1 for r in test_rewards if r > 0) / len(test_rewards)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = '../policies/training_summary.yaml'\n",
    "with open(summary_path, 'w') as f:\n",
    "    yaml.dump(training_summary, f, default_flow_style=False)\n",
    "print(f\"Training summary saved to {summary_path}\")\n",
    "\n",
    "# Test loading the saved model\n",
    "try:\n",
    "    loaded_model = PPO.load(policy_path)\n",
    "    print(\"Policy loading test successful\")\n",
    "    \n",
    "    # Quick test of loaded model\n",
    "    test_obs, _ = env.reset()\n",
    "    test_action, _ = loaded_model.predict(test_obs)\n",
    "    print(f\"Loaded model prediction test: action = {test_action}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Policy loading test failed: {e}\")\n",
    "\n",
    "print(\"\\n=== PPO TRAINING COMPLETE ===\")\n",
    "print(f\"Final average reward: {training_summary['training_results']['final_avg_reward']:.4f}\")\n",
    "print(f\"Test success rate: {training_summary['test_results']['success_rate']:.1%}\")\n",
    "print(f\"Policy saved and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}