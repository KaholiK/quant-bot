name: Model Retraining with Promotion Gates

on:
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retraining even if no improvement'
        required: false
        default: 'false'
        type: boolean
      model_type:
        description: 'Model type to retrain (classifier, ppo, both)'
        required: false
        default: 'both'
        type: choice
        options:
          - both
          - classifier
          - ppo
      optimization_trials:
        description: 'Number of Optuna trials for hyperparameter optimization'
        required: false
        default: '30'
        type: string
  schedule:
    # Run weekly on Sunday at 2 AM UTC (when markets are closed)
    - cron: '0 2 * * 0'

jobs:
  retrain:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hours max
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        lfs: true
        
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install yfinance requests binance-python stable-baselines3[extra] torch
        pip install optuna plotly kaleido  # For hyperparameter optimization and plotting
        
    - name: Download LFS files
      run: git lfs pull
      
    - name: Create output directories
      run: |
        mkdir -p models_new policies_new reports data/cache
        echo "models_new/" >> .gitignore || true
        echo "policies_new/" >> .gitignore || true
        
    - name: Train new classifier model
      if: ${{ github.event.inputs.model_type == 'both' || github.event.inputs.model_type == 'classifier' }}
      run: |
        echo "ðŸ”„ Training new classifier model..."
        python scripts/train_classifier.py \
          --config config.yaml \
          --output-dir models_new \
          --cache-dir data/cache \
          --optimize-params \
          --n-trials ${{ github.event.inputs.optimization_trials || '30' }} \
          --parallel \
          --verbose
          
    - name: Train new PPO policy
      if: ${{ github.event.inputs.model_type == 'both' || github.event.inputs.model_type == 'ppo' }}
      run: |
        echo "ðŸ”„ Training new PPO policy..."
        python scripts/train_ppo.py \
          --config config.yaml \
          --output-dir policies_new \
          --n-envs 8 \
          --total-timesteps 500000 \
          --optimize-params \
          --verbose
          
    - name: Evaluate new models (Walk-Forward Analysis)
      run: |
        echo "ðŸ“Š Evaluating new models with walk-forward analysis..."
        python scripts/evaluate_models.py \
          --old-models-dir models \
          --new-models-dir models_new \
          --old-policies-dir policies \
          --new-policies-dir policies_new \
          --config config.yaml \
          --output reports/evaluation_results.json
          
    - name: Check promotion gates
      id: promotion_check
      run: |
        echo "ðŸšª Checking promotion gates..."
        python scripts/check_promotion_gates.py \
          --results reports/evaluation_results.json \
          --config config.yaml \
          --force ${{ github.event.inputs.force_retrain || 'false' }} \
          --output reports/promotion_decision.json
        
        # Read the promotion decision
        PROMOTE=$(python -c "
        import json
        with open('reports/promotion_decision.json', 'r') as f:
            data = json.load(f)
        print('true' if data.get('promote', False) else 'false')
        ")
        echo "promote=$PROMOTE" >> $GITHUB_OUTPUT
        
        # Read promotion summary for later use
        SUMMARY=$(python -c "
        import json
        with open('reports/promotion_decision.json', 'r') as f:
            data = json.load(f)
        print(data.get('summary', 'No summary available'))
        ")
        echo "summary<<EOF" >> $GITHUB_OUTPUT
        echo "$SUMMARY" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT
        
    - name: Create promotion PR
      if: steps.promotion_check.outputs.promote == 'true'
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "feat: promote new models - automated retrain"
        title: "ðŸš€ Model Promotion: New models passed all gates"
        body: |
          ## Model Promotion Summary
          
          ${{ steps.promotion_check.outputs.summary }}
          
          ### Changes
          - âœ… New models passed all promotion gates
          - ðŸ“Š Walk-forward evaluation results available in reports/
          - ðŸ”„ Automated retraining completed successfully
          
          ### Review Checklist
          - [ ] Review evaluation metrics in reports/
          - [ ] Verify model performance improvements
          - [ ] Check for any unexpected behavior
          - [ ] Approve and merge when ready
          
          *This PR was created automatically by the retraining workflow.*
        branch: model-promotion-${{ github.run_number }}
        delete-branch: true
        
    - name: Move new models to production
      if: steps.promotion_check.outputs.promote == 'true'
      run: |
        echo "ðŸ“¦ Moving new models to production directories..."
        
        # Backup old models
        if [ -d "models" ]; then
          mv models models_backup_$(date +%Y%m%d_%H%M%S)
        fi
        if [ -d "policies" ]; then
          mv policies policies_backup_$(date +%Y%m%d_%H%M%S)
        fi
        
        # Move new models
        if [ -d "models_new" ]; then
          mv models_new models
        fi
        if [ -d "policies_new" ]; then
          mv policies_new policies
        fi
        
        echo "âœ… Models promoted successfully"
        
    - name: Send Discord notification (Success)
      if: steps.promotion_check.outputs.promote == 'true'
      run: |
        if [ -n "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
          python -c "
          import requests, json
          from datetime import datetime
          
          webhook_url = '${{ secrets.DISCORD_WEBHOOK_URL }}'
          
          embed = {
              'title': 'ðŸš€ Model Promotion Successful',
              'description': 'New models have been promoted to production after passing all gates.',
              'color': 0x00ff00,
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'fields': [
                  {'name': 'Workflow', 'value': 'Model Retraining', 'inline': True},
                  {'name': 'Status', 'value': 'Promoted', 'inline': True},
                  {'name': 'Run ID', 'value': '${{ github.run_number }}', 'inline': True}
              ],
              'footer': {'text': 'Quant Bot Retraining'}
          }
          
          payload = {'embeds': [embed]}
          requests.post(webhook_url, json=payload, timeout=10)
          print('Discord notification sent')
          "
        fi
        
    - name: Send Discord notification (Rejected)
      if: steps.promotion_check.outputs.promote == 'false'
      run: |
        if [ -n "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
          python -c "
          import requests, json
          from datetime import datetime
          
          webhook_url = '${{ secrets.DISCORD_WEBHOOK_URL }}'
          
          embed = {
              'title': 'âŒ Model Promotion Rejected',
              'description': 'New models failed to pass promotion gates and were not promoted.',
              'color': 0xff0000,
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'fields': [
                  {'name': 'Workflow', 'value': 'Model Retraining', 'inline': True},
                  {'name': 'Status', 'value': 'Rejected', 'inline': True},
                  {'name': 'Run ID', 'value': '${{ github.run_number }}', 'inline': True}
              ],
              'footer': {'text': 'Quant Bot Retraining'}
          }
          
          payload = {'embeds': [embed]}
          requests.post(webhook_url, json=payload, timeout=10)
          print('Discord rejection notification sent')
          "
        fi
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: retrain-results-${{ github.run_number }}
        path: |
          reports/
          models_new/
          policies_new/
        retention-days: 30
        
    - name: Job summary
      if: always()
      run: |
        echo "## ðŸ¤– Model Retraining Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Run ID:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.promotion_check.outputs.promote }}" = "true" ]; then
          echo "### âœ… Promotion Status: APPROVED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "New models have been promoted to production after passing all gates." >> $GITHUB_STEP_SUMMARY
        else
          echo "### âŒ Promotion Status: REJECTED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "New models failed to meet promotion criteria and were not promoted." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“Š Details" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "${{ steps.promotion_check.outputs.summary }}" >> $GITHUB_STEP_SUMMARY
        
        # Add evaluation results if available
        if [ -f "reports/evaluation_results.json" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“ˆ Key Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          try:
              with open('reports/evaluation_results.json', 'r') as f:
                  results = json.load(f)
              
              print('| Metric | Current | New | Change |')
              print('|--------|----------|-----|--------|')
              
              for metric, values in results.get('comparison', {}).items():
                  current = values.get('current', 0)
                  new = values.get('new', 0)
                  change = new - current
                  change_str = f'+{change:.3f}' if change > 0 else f'{change:.3f}'
                  print(f'| {metric} | {current:.3f} | {new:.3f} | {change_str} |')
          except:
              print('Evaluation results not available')
          " >> $GITHUB_STEP_SUMMARY
        fi
      run: |
        mkdir -p models_new
        mkdir -p policies_new
        mkdir -p reports
        mkdir -p data
        
    - name: Train classifier
      id: train_classifier
      run: |
        echo "Training XGBoost classifier..."
        python scripts/train_classifier.py \
          --output-dir models_new \
          --data-dir data \
          --config config.yaml \
          --cv-folds 5 \
          --n-jobs -1 \
          --save-calibration-plots reports/ \
          --verbose
          
        # Check if new model exists
        if [ -f "models_new/xgb_classifier.joblib" ]; then
          echo "classifier_trained=true" >> $GITHUB_OUTPUT
        else
          echo "classifier_trained=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Train PPO policy
      id: train_ppo
      continue-on-error: true
      run: |
        echo "Training PPO execution policy..."
        python scripts/train_ppo.py \
          --output-dir policies_new \
          --config config.yaml \
          --total-timesteps 100000 \
          --n-envs 4 \
          --save-replay-buffer \
          --verbose
          
        # Check if new policy exists
        if [ -f "policies_new/ppo_policy.zip" ]; then
          echo "ppo_trained=true" >> $GITHUB_OUTPUT
        else
          echo "ppo_trained=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Train meta-filter
      id: train_meta
      run: |
        echo "Training meta-filter model..."
        python scripts/train_meta_filter.py \
          --input-data data/ \
          --output-dir models_new \
          --config config.yaml \
          --threshold-optimization \
          --save-plots reports/
          
        if [ -f "models_new/meta_filter.joblib" ]; then
          echo "meta_trained=true" >> $GITHUB_OUTPUT
        else
          echo "meta_trained=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Evaluate models and apply KPI gates
      id: evaluate
      run: |
        echo "Evaluating new models with KPI gates..."
        python scripts/evaluate_models.py \
          --old-models models/ \
          --new-models models_new/ \
          --config config.yaml \
          --report reports/model_comparison.json \
          --plots reports/
        
        # Apply KPI gates from config
        MEETS_GATES=$(python -c "
        import json, yaml
        try:
            # Load config gates
            with open('config.yaml') as f:
                config = yaml.safe_load(f)
            gates = config['trading']['learning']['gates']
            
            # Load evaluation results
            with open('reports/model_comparison.json') as f:
                results = json.load(f)
            
            new_model = results.get('new_model', {})
            oos_sortino = new_model.get('oos_sortino', 0)
            oos_profit_factor = new_model.get('oos_profit_factor', 0)
            oos_max_dd = new_model.get('oos_max_dd', 1.0)
            
            # Apply gates from config
            sortino_gate = oos_sortino >= gates['oos_sortino_min']
            pf_gate = oos_profit_factor >= gates['oos_profit_factor_min']  
            dd_gate = oos_max_dd <= gates['oos_max_dd_max']
            
            meets_gates = sortino_gate and pf_gate and dd_gate
            
            print(f'Sortino: {oos_sortino:.3f} >= {gates[\"oos_sortino_min\"]} = {sortino_gate}')
            print(f'Profit Factor: {oos_profit_factor:.3f} >= {gates[\"oos_profit_factor_min\"]} = {pf_gate}')
            print(f'Max DD: {oos_max_dd:.3f} <= {gates[\"oos_max_dd_max\"]} = {dd_gate}')
            print(f'Overall: {meets_gates}')
            
            print('true' if meets_gates else 'false')
        except Exception as e:
            print(f'Error evaluating gates: {e}')
            print('false')
        ")
        
        echo "meets_gates=$MEETS_GATES" >> $GITHUB_OUTPUT
        echo "KPI gates result: $MEETS_GATES"
        
    - name: Generate drift report
      run: |
        echo "Generating model drift analysis..."
        python scripts/analyze_drift.py \
          --data-dir data/ \
          --models-dir models/ \
          --output reports/drift_analysis.html \
          --config config.yaml
          
    - name: Upload evaluation artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: retraining-report-${{ github.run_id }}
        path: |
          reports/
          models_new/
          policies_new/
        retention-days: 30
        
    - name: Commit improved models or post rejection summary
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if [ "${{ steps.evaluate.outputs.meets_gates }}" == "true" ] || [ "${{ github.event.inputs.force_retrain }}" == "true" ]; then
          echo "Models meet gates - proceeding with commit"
          
          # Copy new models over old ones
          if [ "${{ steps.train_classifier.outputs.classifier_trained }}" == "true" ]; then
            cp models_new/xgb_classifier.joblib models/
            git add models/xgb_classifier.joblib
          fi
          
          if [ "${{ steps.train_meta.outputs.meta_trained }}" == "true" ]; then
            cp models_new/meta_filter.joblib models/
            git add models/meta_filter.joblib
          fi
          
          if [ "${{ steps.train_ppo.outputs.ppo_trained }}" == "true" ]; then
            cp policies_new/ppo_policy.zip policies/
            git add policies/ppo_policy.zip
          fi
          
          # Create summary report
          echo "# Model Promotion Summary" > reports/summary.md
          echo "" >> reports/summary.md
          echo "**Date:** $(date +%Y-%m-%d)" >> reports/summary.md
          echo "**Status:** APPROVED âœ…" >> reports/summary.md
          echo "" >> reports/summary.md
          echo "## KPI Gates" >> reports/summary.md
          echo "- âœ… Sortino >= 1.2" >> reports/summary.md
          echo "- âœ… Profit Factor >= 1.15" >> reports/summary.md  
          echo "- âœ… Max DD <= 0.06" >> reports/summary.md
          echo "" >> reports/summary.md
          echo "## Models Updated" >> reports/summary.md
          echo "- Classifier: ${{ steps.train_classifier.outputs.classifier_trained }}" >> reports/summary.md
          echo "- Meta-filter: ${{ steps.train_meta.outputs.meta_trained }}" >> reports/summary.md
          echo "- PPO Policy: ${{ steps.train_ppo.outputs.ppo_trained }}" >> reports/summary.md
          
          git add reports/summary.md
          
          # Commit with evaluation summary
          COMMIT_MSG="chore: promote models passing KPI gates $(date +%Y-%m-%d)"
          
          if git diff --staged --quiet; then
            echo "No model changes to commit"
          else
            git commit -m "$COMMIT_MSG"
            git push
          fi
          
        else
          echo "Models rejected by KPI gates - posting summary"
          
          # Create rejection summary
          echo "# Model Rejection Summary" > reports/summary.md
          echo "" >> reports/summary.md
          echo "**Date:** $(date +%Y-%m-%d)" >> reports/summary.md
          echo "**Status:** REJECTED âŒ" >> reports/summary.md
          echo "" >> reports/summary.md
          echo "## KPI Gates Failed" >> reports/summary.md
          echo "Models did not meet the promotion criteria:" >> reports/summary.md
          echo "- Required: Sortino >= 1.2, Profit Factor >= 1.15, Max DD <= 0.06" >> reports/summary.md
          echo "" >> reports/summary.md
          echo "See detailed evaluation in the artifacts." >> reports/summary.md
          
          # This will be uploaded as an artifact but not committed
        fi
        
    - name: Create PR for approved models
      if: steps.evaluate.outputs.meets_gates == 'true'
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "feat: improved models from automated retraining"
        title: "ðŸ¤– Automated Model Update - $(date +%Y-%m-%d)"
        body: |
          ## Automated Model Retraining Results
          
          **Training Date:** $(date +%Y-%m-%d %H:%M:%S UTC)
          **Trigger:** ${{ github.event_name }}
          
          ### Models Updated
          - âœ… Classifier: ${{ steps.train_classifier.outputs.classifier_trained }}
          - âœ… Meta-filter: ${{ steps.train_meta.outputs.meta_trained }}
          - âœ… PPO Policy: ${{ steps.train_ppo.outputs.ppo_trained }}
          
          ### Performance Gates
          - **Meets KPI Gates:** ${{ steps.evaluate.outputs.meets_gates }}
          - **Sortino >= 1.2:** âœ…
          - **Profit Factor >= 1.15:** âœ…  
          - **Max DD <= 0.06:** âœ…
          
          ### Artifacts
          - ðŸ“Š [Evaluation Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - ðŸ“ˆ Calibration plots and performance metrics available in artifacts
          - ðŸ§  Drift analysis included
          
          ### Next Steps
          1. Review the evaluation metrics in the artifacts
          2. Verify the model improvements align with expectations
          3. Merge to deploy the updated models
          4. Monitor live performance for any degradation
          
          ---
          *This PR was created automatically by the retraining workflow.*
        branch: auto-retrain-${{ github.run_id }}
        delete-branch: true
        
    - name: Send notification
      if: always()
      run: |
        # Send Discord notification if webhook is configured
        if [ -n "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
          python -c "
          import requests, json, os
          
          webhook_url = '${{ secrets.DISCORD_WEBHOOK_URL }}'
          
          status = 'âœ… SUCCESS' if '${{ job.status }}' == 'success' else 'âŒ FAILED'
          meets_gates = '${{ steps.evaluate.outputs.meets_gates }}'
          
          if meets_gates == 'true':
              title = f'ðŸ¤– Model Retraining {status} - Models Promoted'
              color = 0x00ff00
          else:
              title = f'ðŸ¤– Model Retraining {status} - Models Rejected'
              color = 0xff6600
          
          embed = {
              'title': title,
              'color': color,
              'fields': [
                  {'name': 'Classifier', 'value': '${{ steps.train_classifier.outputs.classifier_trained }}', 'inline': True},
                  {'name': 'PPO Policy', 'value': '${{ steps.train_ppo.outputs.ppo_trained }}', 'inline': True},
                  {'name': 'Meta-filter', 'value': '${{ steps.train_meta.outputs.meta_trained }}', 'inline': True},
                  {'name': 'Meets Gates', 'value': meets_gates, 'inline': True},
                  {'name': 'Repository', 'value': '${{ github.repository }}', 'inline': True},
                  {'name': 'Run ID', 'value': '${{ github.run_id }}', 'inline': True}
              ],
              'timestamp': '$(date -u +%Y-%m-%dT%H:%M:%SZ)'
          }
          
          payload = {'embeds': [embed]}
          requests.post(webhook_url, json=payload)
          "
        fi